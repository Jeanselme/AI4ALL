{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important part of data science is to understand your data. To do so, it is interesting to visualize them and explore the link between the different variables.\n",
    "\n",
    "This notebook will introduce some visualization techniques and mathematically quantify the link between different variables: Pearson Correlation. You will also discover what is Principal Components Analysis (PCA).\n",
    "\n",
    "As in the previous notebook, we will describe the square data example and then you will have to explore the house price dataset by yourself. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd # You begin to know this library\n",
    "import numpy as np # This one too\n",
    "import matplotlib.pyplot as plt # This is a new one !\n",
    "from mpl_toolkits.mplot3d import Axes3D # To plot in 3d\n",
    "import seaborn as sns # And a second new one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Matplotlib](https://matplotlib.org/contents.html) will allow you to make clear visualization of your data.\n",
    "\n",
    "[Seaborn](https://seaborn.pydata.org/) makes it look nicer and have some more tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Square example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "squares = pd.read_csv('data/squares.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before visualizing, it is always interesting to hypothesize what we should observe. This way it is simpler to detect errors but also counterintuitive behavior of the data.\n",
    "\n",
    "So for this toy dataset, I expect the first point to have coordinates between 0 and 1 on x and y axes (because I built the dataset this way !) in a totally random distribution (close to uniform, meaning each point within has the same probability of being sampled). And the area of the square should not depend on the first point, so the area color should appear also random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure() # That allows you to create a figure\n",
    "squares.plot.scatter(x='x0', y='y0', c='area', colormap='viridis') \n",
    "# c is obtional, it is in order to add another dimension\n",
    "plt.show() # It allows to display what you have drawn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check the uniform distribution, it can be interesting to display the histogram which reflects the empirical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "squares.x0.plot.hist(bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the side of the square uniformly distributed ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute side\n",
    "\n",
    "# Display the histogram\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visual verification is not really statistical, it exists some test to be sure of this assumption that we will see later for normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# Plot the histogram of the area\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the square of the size is not unifromly distributed.\n",
    "\n",
    "If you want to do some statistic you can look at the following computation, otherwise skip it and let's verify that the distribution is $$f_{X^2}(x) = \\dfrac1{2\\sqrt{x}}$$\n",
    "\n",
    "Here is the math that you can skip :)\n",
    "$$F_{X^2}(x) = \\mathbb{P}(X^2 \\leq x) = \\mathbb{P}(X \\in [0,\\sqrt{x}]) = \\sqrt{x}$$\n",
    "$$f_{X^2}(x) = \\dfrac{dF_{X^2}(x)}{dx} = \\dfrac1{2\\sqrt{x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listX = np.linspace(10**-2, 1, 100)\n",
    "listY = 1/(2*np.sqrt(listX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(listX, listY) # This function plot the points of coordinates x, y\n",
    "squares.area.plot.hist(bins = 100, normed = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this plot, what is the theoritical mean value of areas that we should observe ? Is it coherent with np.mean(squares.area) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia defines the [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) as \"a **statistical procedure** that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of **linearly uncorrelated variables** called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple words, PCA allows you to compute combination of the different features that explain the change in the data. This way, you can select the few first dimensions in order to have a good visualization of your data, or a strong compression of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of the PCA because it would not be easy to compute it by hand !\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "# Fit the pca to the given data and return the transformed data\n",
    "transformed = pca.fit_transform(squares) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.barplot(x = np.arange(len(pca.explained_variance_)), \n",
    "            # np.arange creates a list of number \n",
    "            # from 0 to the given value\n",
    "            y = np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"Principal Component\") \n",
    "plt.ylabel(\"Cumulative Explained variance\")\n",
    "plt.axhline(0.95, linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dotted line represent the 95% of explained variance which is a standard threshold for stopping the algorithm: in this case, we can use only 4 dimensions to represent really accurately the data.\n",
    "\n",
    "This is coherent with what we can expect, there is a lot of redundancy for a square by repeating the coordinates of each points. Can you think about an easy representation in 4 dimensions for a square ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the PCA has found : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayPC(pca, dataset, numberDim = 4):\n",
    "    \"\"\"\n",
    "        Displays the first numberDim components of the \n",
    "        given pca computed on the given dataset\n",
    "        \n",
    "        No worries, we will explain for loops later\n",
    "    \"\"\"\n",
    "    for dim in range(numberDim):\n",
    "        print(\"PC {} :\".format(dim) \n",
    "              + '+'.join([\" {:.2f} {:s} \".format(pc, c)\n",
    "                          for pc, c in zip(pca.components_[dim], \n",
    "                                           dataset.columns)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayPC(pca, squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think that this representation is much more complex that the one that you were thinking about ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is still interesting to display the data in this representation. But because of the randomness of our dataset, we don't expect to see too much structure in the cloud of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(transformed[:, 0], transformed[:, 1], transformed[:, 2],\n",
    "           c=transformed[:, 3], cmap=plt.cm.viridis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation is a **statistical relationship involving dependence**(linear relationship with each other).\n",
    "\n",
    "Correlations are useful because they can indicate a predictive relationship that can be **exploited in practice**. For example, an electrical utility may produce less power on a mild day based on the correlation between electricity demand and weather. In this example, there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling. However, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (**correlation does not imply causation !**).\n",
    "\n",
    "Here is an example extracted from the website [Spurious Correlation](http://tylervigen.com/spurious-correlations)\n",
    "\n",
    "![Correlation](./img/SpuriousCorrelation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sometimes possible to explain these correlations by a shared cause, but there is no causal connection between the two features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have any assumption of what we should observe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = squares.corr()\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hum ... Not really nice to look at, seaborn allows us to make a much better representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(correlation, vmin=-1, vmax=1, cmap=\"bwr\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn also allows you to observe the distribution in parallel to the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.pairplot(squares, kind=\"reg\") \n",
    "# You can limit the number of plots by using vars=[\"var1\", ...]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this observation, can you make an assumption on how I created this dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say about the distribution of the other points ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that you have played with these tools, let's apply them to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the mean, median, min, max prices of sale and the standard deviation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the two following values:\n",
    "mean = \n",
    "std = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would it mean if the std was 0 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the market stable over the studied period ? In order to answer this question, make a comparaison of the SalePrice with the YrSold. Why is it important to look at this point ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most correlated variable with the SalePrice ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the price normally distributed ?   \n",
    "Define the function `normalDistribution`, the bell curve which is defined by :\n",
    "$$N(x, \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} $$\n",
    "with $\\mu$ the mean and $\\sigma$ the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalDistribution(points, mean, std):\n",
    "    \"\"\"\n",
    "        Computes the normal distribution of mean and std\n",
    "        for the given points\n",
    "    \"\"\"\n",
    "    values = \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# Plot the distribution of SalePrice\n",
    "houses.SalePrice.plot.hist(bins = 100, normed=True)\n",
    "# Plot the normal distribution\n",
    "points = np.linspace(houses.SalePrice.min(), houses.SalePrice.max(), 10**4)\n",
    "plt.plot(points, normalDistribution(points, mean, std), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you conclude ? Here a statistical tool would be great ! You will look on how to correct it in the advance notebook that deals with improvement of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericalHouses = houses.select_dtypes(include=\"number\")\n",
    "numericalHouses = numericalHouses.fillna(-1) \n",
    "# Replace all absent data with -1\n",
    "# => We will do a much better analysis in the next notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reduce the data dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "transformed = pca.fit_transform(numericalHouses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data on the two first components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis the two first components => Look at the maximum values\n",
    "# How much dimensions do you need to explain 95 % of the variability\n",
    "pca.components_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recompute a PCA without the SalePrice but plot the SalePrice as the\n",
    "c color in order to know if the first components are linked with \n",
    "this price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}