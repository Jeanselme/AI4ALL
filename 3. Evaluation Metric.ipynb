{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces how to measure the performances of your models and why it is necessary to validate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building a model, we need to choose what are the properties of the model that we want to stress and how to measure its performance. For a regression problem, it is possible to measure the distance between the predictions and the ground truth. The best model will be the one that minimize this error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple idea is to compute the difference between the predictions and the groundTruth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete\n",
    "def loss(predictions, groundTruth):\n",
    "    \"\"\"\n",
    "        Computes the difference between each prediction and its groundTruth\n",
    "        And returns the mean over all the sample\n",
    "    \"\"\"\n",
    "    error = \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the limit of this loss ? Give an example that has an error of zero but none of the predictions is even close to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete\n",
    "predictions = [1, -1, 0, 5, -5, 3, -3]\n",
    "groundTruth = [-1, 1, 0, 5, -5, 3, -3]\n",
    "assert loss(predicitons, groundTruth) == 0, \"The loss is not null\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to correct this problem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other aspects of the model do you think important ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coefficient of determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "An enhanced version of this loss is the coefficient of determination which measures how the model replicates the observed outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we minimize the loss on the training set, we will certainly have a problem of **overfitting**. This section is absed on the medium article: [Memorizing is not learning!\u200a\u2014\u200a6 tricks to prevent overfitting in machine learning](https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a simple example: build a model thanks to a `if` `then` `else` condition.\n",
    "In python, it works as follow:\n",
    "\n",
    "    if condition:\n",
    "        # If the condition is True executes this part of the code \n",
    "        code ...\n",
    "    elif secondCondition:\n",
    "        # If secondCondition is True executes this part \n",
    "        # (you can repeat elif as many time as you need)\n",
    "        code ...\n",
    "    else:\n",
    "        # If none of the previous conditions is True\n",
    "        code ...\n",
    "        \n",
    "Now let's built our first simplified model: let's say that each house is represented as follow: colorWindow, yearConstruction, squareFeet and you have to predict a price. Here is the dataset that you will use for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingHouses = [[\"blue\", 2010, 700], [\"blue\", 2015, 550], [\"green\", 2015, 700]]\n",
    "trainingSalePrices = [100000, 150000, 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete\n",
    "# Here your first model\n",
    "def model(house):\n",
    "    colorWindow, yearConstruction, SquareFeet = house\n",
    "    if ...:\n",
    "        ...\n",
    "    elif :\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [model(trainingHouse) for trainingHouse in trainingHouses]\n",
    "assert lossCorrected(predictions, trainingSalePrices) == 0, \"Your model seems to be uncorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A real estate agent comes to you and wants your estimation of a new house that he has to sell. What is your estimation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newHouse = [\"blue\", 2015, 700]\n",
    "# Your estimation ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is really unpleased by your answer ! Your model has not been able to generalized to new data: it has overfitted !\n",
    "\n",
    "\"The word overfitting refers to a model that **models the training data too well**. Instead of learning the general distribution of the data, the model learns the expected output for every data point. \n",
    "\n",
    "This is the same a memorizing the answers to a maths quizz instead of knowing the formulas. Because of this, the model cannot generalize. Everything is all good as long as you are in familiar territory, but as soon as you step outside, you\u2019re lost.\n",
    "\n",
    "The tricky part is that, at first glance, **it may seem that your model is performing well** because it has a very small error on the training data. However, as soon as you ask it to predict new data points, it will fail.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Overfitting / Underfitting](https://cdn-images-1.medium.com/max/1600/1*SBUK2QEfCP-zvJmKm14wGQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to measure it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to detect it, you want to analyze the performance on data that **it has never seen before** ! This will allow you to measure its capacity to **generalize** to new data. However it is hard to have new data to test your model for each new model, so to overcome this issue, you **split** your dataset into **training** and **testing** sets.\n",
    "\n",
    "The training set has to be large enough to generalize and the testing set **representative** enough of the dataset in order to measure how **accurate** is your model (usually you randomize this testing set to have a overview of all the data). The following table allows you to detect when your model is overfitting to the data.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*3XvSvKfde8u89TMwjkz3kg.png\" alt=\"Detection\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's split our original dataset in two sets (80% for training and 20% for the testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select a subset of a list in Python, you have to use `[]`\n",
    "\n",
    "    list = [10,23,52,32,44]\n",
    "    list[0] # Returns the value 10\n",
    "    list[:2] # Returns the list [20,23]\n",
    "    \n",
    "In order to take 20% of this list, you have to code\n",
    "    \n",
    "    length = len(list)\n",
    "    stop = 0.2 * length # Takes only 20%\n",
    "    stop = int(stop) # Changes the type \n",
    "    # (because it does not make sense to take the float of a list)\n",
    "    twentyPercentFirst = list[:stop] # Returns twenty first percents\n",
    "    twentyPercentLast = list[-stop:] # Returns last twenty percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete\n",
    "# Opens data\n",
    "houses = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffles the dataset\n",
    "np.random.shuffle(houses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete\n",
    "# Splits the data\n",
    "train, test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now be able to detect if your models overfit but how to prevent this behavior ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to prevent it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is limited in the number of information it can \"memorize\", more data you will have more there is a chance that it overfit. In order to do so, you can **increase the dataset with new datapoints** (however it is sometimes hard, even impossible). \n",
    "\n",
    "In this case, you can **create** new datapoints. In computer vision, it is usual to add noise, reframe images in order to make the model more robust to perturbation that can be observed in real case (the noise has to be realisitc, it is why the point of view of an expert is valuable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think that it is possible to increase our current dataset ? If so, how would you do ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*vuZxFMi5fODz2OEcpG-S1g.png\" alt=\"Overfitting / Underfitting Loss\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model has different caracteristics that can change how much information it can memorize. Reducing the complexity of your model, by variating the different parameters can increase the performances of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, in our previous model, you can use only use two if conditions. The performances on the training will perhaps be slightly lower but the goal is to find the right generalization of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring each possibilities by changing the different hyperparameters can be time consuming. It is more efficient to find a way to integrate it to the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last solution that we will present is to not try to minimize the loss but to add a penalty that will quantify the complexity of the model. So if the model becomes too complex, the penalty will increase and the sum of the loss and this **regularizor** will also increase. The best model will the one that minimize the loss but also minimize the penalty.\n",
    "\n",
    "$$ \\min Loss(model(data), truth) + complexity(model)$$\n",
    "\n",
    "This solution is faster than the exploration of the hyperparameters space because it intervenes directly during the training process and does not require to compute multiple models with different hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real application, it is usual to take advantage of the different techniques to create the best generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}