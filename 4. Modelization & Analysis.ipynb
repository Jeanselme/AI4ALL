{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will build three models and analyze them !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open training data\n",
    "houses = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train / test group\n",
    "test, train = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because some models can be sensible to the scale of the data, it is important to force the data to have a common scale on each dimension. To ensure that, we can force each feature to have a mean of 0 and a std deviation of 1 by computing;\n",
    "$$X_{norm} = \\frac{X - \\mu}{\\sigma}$$ \n",
    "with $\\mu$ the mean of $X$ and $\\sigma$ its standrad deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyNormalization(data, mean, std):\n",
    "    \"\"\"\n",
    "        Given a mean and std, normalize the data\n",
    "    \"\"\"\n",
    "    normalizedData = \n",
    "    return normalizedData\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "        Normalize the given data\n",
    "    \"\"\"\n",
    "    mean =\n",
    "    std = \n",
    "    normalizedData = applyNormalization(data, mean, std)\n",
    "    return normalizedData, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now iterate this normalization over each column and save the mean and std in a dictionary. These values are important in order to reverse the transformation and also to apply the exact same transformation to a new set of data. \n",
    "\n",
    "It is important to compute these values over the training set and to apply the same transformation on all the other sets that we will use in order to make them coparable to the data used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedTransformation = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, complete the following loop in order to iterate over each column of the training dataset, normalize each of them and save the parameters that you have computed in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = \n",
    "for column in columns:\n",
    "    # Computes normalization\n",
    "    normalizedData, mean, std =\n",
    "    # Save the normalization in a dictionary\n",
    "    # Actually we save a dictionary in a dictionary !\n",
    "    savedTransformation[column] = {\"std\": std, \"mean\": mean} \n",
    "    # Write the new data in the dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, verify that the mean and standard deviation of the new columns are respectively 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, apply the same transformation than before on each column of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, everything is ready for the first models !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sklearn](http://scikit-learn.org/stable/documentation.html) proposes a lot of models for classification, regression, clustering ... And much more ! In addition to an optimize code, it offers a standard way of training and testing models. For each model, you will have to do :\n",
    "\n",
    "    from sklearn import model\n",
    "    m = model() # add the parameter n_jobs=-1 for parallel\n",
    "    m.fit(trainData, trainLabels)\n",
    "    m.score(testData, testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which part of the data will you use for features and which one is the label ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, trainLabels =\n",
    "testData, testLabels ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression tries to fit a line to the distribution of points. It is simplier to look at it in two dimensions. The following example plots the linear regression if we tries to predict the salePrice, only with OverallQual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.lmplot(x = \"OverallQual\", y = \"SalePrice\", data = houses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line would be the results of the linear regression on this given example. However, it is possible to compute it in multiple dimensions at the same time !\n",
    "\n",
    "Do you already see an improvement to this model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model from sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the first model and display the score on the training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression puts a weight on the different features in order to compute the best prediction. The resulting model is easy to interpret because it is only necessary to look at the weight on each features and take the one with the most important weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, on the following model, which features is the most and least important features:\n",
    "    \n",
    "    price = 0.6 * numberRoom - 0.9 * yearConstruction \n",
    "        + 0.7 * squarefeet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayImportanceFeatures(dataset, importance):\n",
    "    \"\"\"\n",
    "        Display the importance of each features on\n",
    "        the given dataset\n",
    "    \"\"\"\n",
    "    sort = np.argsort(importance)[::-1]\n",
    "    sort = sort[:10]\n",
    "    plt.figure()\n",
    "    sns.barplot(y = dataset.columns[sort], x = importance[sort], orient='h')\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this value with numpy\n",
    "importance = lr.coef_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayImportanceFeatures(houses, importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has interesting results, but we should continue to explore different models in order to see if we can have reduce the loss. \n",
    "\n",
    "Which is the most important feature in order to predict the sale price of the house ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have the following house, what would you have to change in your house to increase the prediction of $10,000 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yourHouse = test.iloc[48]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to analyze the impact of two features on the prediction of the price of your house. For a linear regression, this will be a plan because linear regression tries to modelize each parameters with a simple line, however for more complex model it could have a different shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to help you, here is a function that will plot the impact in 3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plotInfluence(model, col1, col2, yourHouse, grid = 50): \n",
    "    \"\"\"\n",
    "        Plots the evolution of the model by variating\n",
    "        the value of col1, col2\n",
    "    \"\"\"\n",
    "    x_values = np.linspace(houses[col1].min(), houses[col1].max(), grid)\n",
    "    y_values = np.linspace(houses[col2].min(), houses[col2].max(), grid)\n",
    "    copy = yourHouse.copy()\n",
    "    \n",
    "    toPredict = []\n",
    "    for x in x_values:\n",
    "        for y in y_values:\n",
    "            copy[col1] = x\n",
    "            copy[col2] = y\n",
    "            toPredict.append(copy.copy().values.reshape((1, -1)))\n",
    "            \n",
    "    %matplotlib inline\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    X, Y = np.meshgrid(x_values, y_values)\n",
    "    ax.plot_surface(X, Y, np.reshape(model.predict(np.concatenate(toPredict)),(50, 50)))\n",
    "    ax.set_xlabel(col1)\n",
    "    ax.set_ylabel(col2)\n",
    "    ax.set_zlabel(\"Predictions\")\n",
    "    plt.show()\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to select\n",
    "col1 = \"OverallQual\"\n",
    "col2 = \"LotArea\"\n",
    "plotInfluence(lr, col1, col2, yourHouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting second model is a decision tree. Here is an example from the [notebook](https://www.kaggle.com/dansbecker/underfitting-overfitting-and-model-optimization):\n",
    "![Tree](http://i.imgur.com/R3ywQsR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The successive decision allows to fit more complex curve than a single line to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the following model and look how changing the two most important features differs from the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2d \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with a lot of parameters, we will only look at the max_depth parameters of the tree (how many successive comparisons can be made between the root and the decision), which is how high can be the tree. \n",
    "How much the number of nodes increase as each step (if you suppose that the tree is complete) ?\n",
    "If you don't have an intuition, try to create a loop that computes it for you !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 1 # The root\n",
    "for depth in range(10):\n",
    "    print(depth, nodes)\n",
    "    nodes = nodes + "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you expect to be the performance when the size increase ?\n",
    "\n",
    "Train a model for different depths and then plot the performance over the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = list(range(5, 100, 15))\n",
    "perfTest, perfTrain = [], []\n",
    "for depth in depths:\n",
    "    print(depth)\n",
    "    # Trains model\n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.\n",
    "    \n",
    "    # Computes score\n",
    "    scoreTest = \n",
    "    scoreTrain = \n",
    "    \n",
    "    # Append the scores in the list\n",
    "    perfTest.append(scoreTest)\n",
    "    perfTrain.append(scoreTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the evolution of the scores given the depth using the lists : `depths`, `perfTest` and `perfTrain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should we look at only one tree ? Let's explore a clever extrapolation of the decision tree: the **random forest** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is an extrapolation of the tree, you can think it as a forest of tree. Some little tricks have to be applied to make this model less sensitive to overfitting. At the end, that will be a lot of trees, that will give a predictions and the final results will be the average of all these predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# For this model it is interesting to use n_estimators=100 \n",
    "# Bonus: A better analysis can be made like in the previous point to see when the model begins to overfit\n",
    "rf = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the importance of the different featuers, is it coherent with the previous models ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayImportanceFeatures(houses, rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of the two first features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the impact of variating the two first components on your house. What has changed compare to the previous models ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = \"...\"\n",
    "col2 = \"...\"\n",
    "plotInfluence(rf, col1, col2, yourHouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare these different model, it is important to compare their performances on the training set. However, if your goal is to explain to a seller which aspects of his house he should change, which model would you use ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you wanna try to participate to the Kaggle ? \n",
    "\n",
    "To do so, you have to make a prediction for the houses present in the file kaggle.csv and finally format it by selecting the `Id` and `SalePrice` and save it in afile (an example is given in the file `sample_submission.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}